{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1f95LnlhFI6R17cIcJASiE98AfGy3uuCr",
      "authorship_tag": "ABX9TyOz3l/AtqRV2uHGzuzw8w/w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jx-dohwan/BERT-FP_MRS/blob/main/post_pretrain/Post_Pretrain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Post-training"
      ],
      "metadata": {
        "id": "cVenUKAHi5-z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜"
      ],
      "metadata": {
        "id": "jH75PgrgjBGJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqX3WxLyi1H3",
        "outputId": "ce798a47-9f68-4c58-d24e-28ee9e144b98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.13.1+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.4.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers==4.25.1 in /usr/local/lib/python3.8/dist-packages (4.25.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers==4.25.1) (23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers==4.25.1) (0.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.25.1) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers==4.25.1) (3.9.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.25.1) (2022.6.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers==4.25.1) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers==4.25.1) (2.25.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers==4.25.1) (0.13.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers==4.25.1) (4.64.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.25.1) (4.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.25.1) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.25.1) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.25.1) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.25.1) (1.24.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch\n",
        "!pip install transformers==4.25.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y21vmEjKjMyc",
        "outputId": "2f3c62f1-942e-4ca1-ae5f-1061f35ed4c4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ë°ì´í„° ë‹¤ìš´ë¡œë“œ"
      ],
      "metadata": {
        "id": "reklAI1YjXjy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ìŠ¤ë§ˆì¼ ìŠ¤íƒ€ì¼ ë°ì´í„°ì„¸íŠ¸(for ëŒ€í™” ë° ìŠ¤íƒ€ì¼ íŠ¸ëœìŠ¤í¼)"
      ],
      "metadata": {
        "id": "sVoUGEDzjZvB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/smilegate-ai/korean_smile_style_dataset\n",
        "# https://corpus.korean.go.kr/\n",
        "# https://aihub.or.kr/aihub-data/natural-language/about"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22U2-AHDjU_y",
        "outputId": "52b2fa35-e418-42cf-b1ba-5397cda83533"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'korean_smile_style_dataset' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/korean_smile_style_dataset/smilestyle_dataset.tsv', sep='\\t')\n",
        "df.to_csv('./korean_smile_style_dataset/smile.csv', index=False)"
      ],
      "metadata": {
        "id": "eXSSXMFxjunK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head -3 './korean_smile_style_dataset/smile.csv'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0v7T4wxkJwc",
        "outputId": "64a16dbd-b14a-4fc2-93e4-17d1319b856a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "formal,informal,android,azae,chat,choding,emoticon,enfp,gentle,halbae,halmae,joongding,king,naruto,seonbi,sosim,translator\n",
            "ì•ˆë…•í•˜ì„¸ìš”. ì €ëŠ” ê³ ì–‘ì´ 6ë§ˆë¦¬ í‚¤ì›Œìš”.,ì•ˆë…•! ë‚˜ëŠ” ê³ ì–‘ì´ 6ë§ˆë¦¬ í‚¤ì›Œ.,íœ´ë¨¼. ë°˜ê°‘ë‹¤. ì•ˆë“œë¡œì´ë“œëŠ”. ê³ ì–‘ì´. 6ë§ˆë¦¬. ì†Œìœ ì¤‘.,ì•„ì´ê³  ì•ˆë…•í•˜ì‹­ë‹ˆê¹Œ~ ë‚˜ëŠ” ê·¸ëƒ¥ ê³ ì–‘ì´ 6ë§ˆë¦¬ í‚¤ìš°ê³  ìˆëŠ” ì‚¬ëŒì´ì—¬,í•˜ì‰ã…‹ã…‹ ë‚˜ ë–¼ê±¸ë£© 6ë§ˆë¦¬ í‚¤ìš´ë‹¤!,ã…ã…‡ ë‚˜ ì£¼ì¸ë‹˜ 6ë§ˆë¦¬ ëª¨ì‹¬ ã…‹ã…‹,ì•ˆë…•!! >< ë‚˜ëŠ” ê³ ì–‘ì´ğŸ˜ºë¥¼ â ë§ˆë¦¬ í‚¤ìš°ê³ ìˆì–´!! 0_0,ì•ˆë…•ì•ˆë…•~! ë‚˜ ê³ ì–‘ì´ 6ë§ˆë¦¬ë‚˜ í‚¤ìš´ë‹¤? ì™„ì „ ëŒ€ë°•ì´ì§•~,\"ì•ˆë…•í•˜ì‹­ë‹ˆê¹Œ,, ì €ëŠ” ê³ ì–‘ì´ 6ë§ˆë¦¬ í‚¤ì›ë‹ˆë‹¤.\",ì•ˆë…•í•˜ì‹ ê°€~... ë‚œ ì§€ê¸ˆ ê³ ì–‘ì´ë¥¼ 6ë§ˆë¦¬ í‚¤ìš°ê³  ìˆë‹¤ë„¤,í•˜ìœ  ì‹œë²Œê²ƒ ê´­ì´ë†ˆ 6ë§ˆë¦¬ í‚¤ìš°ëŠ”ë° í˜ë“¤ì–´ ì£½ê²Ÿë„¤,ì•ˆë…•í•˜ëƒ ã…¡ã…¡ ë‚˜ ì”¹ëƒ¥ì´ 6ë§ˆë¦¬ë‚˜ í‚¤ìš´ë‹¤ í•˜;,ë°˜ê°‘ì†Œ. ì§ì€ ê³ ì–‘ì´ë¥¼ 6ë§ˆë¦¬ë‚˜ í‚¤ìš°ì˜¤.,ì•ˆë…•í•˜ëƒë‹ˆê¹! ë‚œ ê³ ì–‘ì´ë¥¼ 6ë§ˆë¦¬ í‚¤ìš°ê³ ìˆë‹¤ë‹ˆê¹!,ì•ˆë…•í•˜ì‹œì˜¤! ì†Œì¸ì€ ê³ ì–‘ì´ë¥¼ 6ë§ˆë¦¬ í‚¤ìš°ê³  ìˆì†Œ!,ì•ˆë…•â€¦ ë‚œ ê³ ì–‘ì´ 6ë§ˆë¦¬ í‚¤ì›Œ ã… ã… ,ë°˜ê°€ìš´. ë‚˜ëŠ” 6ë§ˆë¦¬ì˜ ê³ ì–‘ì´ë¥¼ ì†Œì§€í•˜ê³  ìˆë‹¤.\n",
            "ê³ ì–‘ì´ë¥¼ 6ë§ˆë¦¬ë‚˜ìš”? í‚¤ìš°ëŠ”ê±° ì•ˆ í˜ë“œì„¸ìš”?,ê³ ì–‘ì´ë¥¼ 6ë§ˆë¦¬ë‚˜? í‚¤ìš°ëŠ”ê±° ì•ˆ í˜ë“¤ì–´?,ê³ ì–‘ì´. 6ë§ˆë¦¬. ì–‘ìœ¡. ë²ˆê±°ë¡œìš´ê°€.,ì•„ë‹ˆ ë¬´ìŠ¨ ê³ ì–‘ì´ë¥¼ 6ë§ˆë¦¬ë‚˜? ê±° í‚¤ìš°ëŠ” ê±° ì•ˆ í˜ë“ ê°€?,ì—¥? 6ë§ˆë¦¬ë‚˜? ì•ˆí˜ë“¬?ã…‹ã…‹ã…‹ã…‹,6ë§ˆë¦¬? ì—ë°”ì•„ë‹ˆëƒ ì•ˆ í˜ë“¦?,ê³ ì–‘ì´ë¥¼ 6ë§ˆë¦¬ë‚˜?!! w(ï¾ŸĞ”ï¾Ÿ)w í‚¤ìš°ëŠ”ê±° ì•ˆ í˜ë“¬?? (âŠ™_âŠ™;),ê³ ì–‘ì´ë¥¼ 6ë§ˆë¦¬ë‚˜? ì™„ì „ ëŒ€ë°•~ í‚¤ìš°ëŠ” ê±° ì•ˆ í˜ë“¤ì–´?!,\"ê³ ì–‘ì´ë¥¼ 6ë§ˆë¦¬ë‚˜ í‚¤ìš°ì‹­ë‹ˆê¹Œ? ì•ˆ í˜ë“œì‹ ì§€,,\",ê³ ì–‘ì´ë¥¼ 6ë§ˆë¦¬ë‚˜? í‚¤ìš°ëŠ”ê±° í˜ë“¤ì§€ ì•ŠëŠ”ê°€?,ë‹ˆê¸°ëŸ´ í„¸ë§Œ ë‚ ë¦¬ëŠ” ê±° í‚¤ìš°ê¸° ì•ˆ í˜ë“¤ë°?,ì•„ë‹ˆ ê³ ì–‘ì´ë¥¼ 6ë§ˆë¦¬ë‚˜? ì•ˆí˜ë“œëƒ?,ê³ ì–‘ì´ë¥¼ 6ë§ˆë¦¬ë‚˜? í‚¤ìš°ëŠ”ê²Œ ìˆ˜ê³ ìŠ¤ëŸ½ì§„ ì•Šì†Œ?,ê³ ì–‘ì´ë¥¼ 6ë§ˆë¦¬ë‚˜? í‚¤ìš°ëŠ”ê±° í˜ë“¤ì§€ ì•Šëƒë‹ˆê¹?,ê³ ì–‘ì´ë¥¼ 6ë§ˆë¦¬ë‚˜ í‚¤ìš°ê³  ìˆëŠ” ê²ƒì´ì˜¤? í˜ë“¤ì§€ ì•Šì†Œ?,ê³ ì–‘ì´..6ë§ˆë¦¬ë‚˜? ã…  í‚¤ìš°ëŠ”ê±´ í˜¹ì‹œ ì•ˆí˜ë“¤ì–´..?,6ë§ˆë¦¬ì˜ ê³ ì–‘ì´? ë‹¹ì‹ ì€ ê·¸ë“¤ë¡œë¶€í„° ì§€ì¹˜ì§€ ì•ŠìŠµë‹ˆê¹Œ?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ë°ì´í„° ê°€ê³µí•˜ê¸°"
      ],
      "metadata": {
        "id": "ZkmRR95TkZWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "data_path = './korean_smile_style_dataset/smile.csv'\n",
        "f = open(data_path, 'r')\n",
        "rdr = csv.reader(f)\n",
        "\n",
        "for line in rdr:\n",
        "    print(line)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpVJsdF6kOya",
        "outputId": "04df9196-d9e9-4090-b956-176bbcad8b57"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['formal', 'informal', 'android', 'azae', 'chat', 'choding', 'emoticon', 'enfp', 'gentle', 'halbae', 'halmae', 'joongding', 'king', 'naruto', 'seonbi', 'sosim', 'translator']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ë°ì´í„° ì €ì¥\n",
        "def split(session):\n",
        "    final_data = []\n",
        "    split_session = []\n",
        "    for line in session:\n",
        "        split_session.append(line)\n",
        "        final_data.append(split_session[:])\n",
        "    return final_data\n",
        "\n",
        "f = open(data_path, 'r')\n",
        "rdr = csv.reader(f)\n",
        "\n",
        "\"\"\" ì„¸ì…˜ ë°ì´í„° ì €ì¥í•  ê²ƒ\"\"\"\n",
        "session_dataset = []\n",
        "session = []\n",
        "\n",
        "\"\"\" ì‹¤ì œ ë°ì´í„° ì €ì¥ ë°©ì‹ \"\"\"\n",
        "for i, line in enumerate(rdr):\n",
        "    if i == 0:\n",
        "        header = line\n",
        "    else:\n",
        "        utt = line[0] #formal\n",
        "        if utt.strip() != '':\n",
        "            session.append(utt)\n",
        "        else:\n",
        "            \"\"\" ì„¸ì…˜ ë°ì´í„° ì €ì¥ \"\"\"\n",
        "            session_dataset.append(session)\n",
        "            session = []\n",
        "\"\"\" ë§ˆì§€ë§‰ ì„¸ì…˜ ì €ì¥ \"\"\"\n",
        "session_dataset.append(session)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "oqqZYKXalM_D"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "session_dataset[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvmiiWL-mRxa",
        "outputId": "64d3dd27-707e-4cb0-fa0e-39e5d37cfbdf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ì•ˆë…•í•˜ì„¸ìš”. ì €ëŠ” ê³ ì–‘ì´ 6ë§ˆë¦¬ í‚¤ì›Œìš”.',\n",
              " 'ê³ ì–‘ì´ë¥¼ 6ë§ˆë¦¬ë‚˜ìš”? í‚¤ìš°ëŠ”ê±° ì•ˆ í˜ë“œì„¸ìš”?',\n",
              " 'ì œê°€ ì›Œë‚™ ê³ ì–‘ì´ë¥¼ ì¢‹ì•„í•´ì„œ í¬ê²Œ í˜ë“¤ì§„ ì•Šì•„ìš”.',\n",
              " 'ê°€ì¥ ë‚˜ì´ê°€ ë§ì€ ê³ ì–‘ì´ê°€ ì–´ë–»ê²Œ ë¼ìš”?',\n",
              " 'ì—¬ì„¯ ì‚´ì…ë‹ˆë‹¤. ê°ˆìƒ‰ ê³ ì–‘ì´ì—ìš”.',\n",
              " 'ê·¸ëŸ¼ ê°€ì¥ ì–´ë¦° ê³ ì–‘ì´ê°€ ì–´ë–»ê²Œ ë¼ìš”?',\n",
              " 'í•œ ì‚´ì…ë‹ˆë‹¤. ì‘ë…„ì— ë¶„ì–‘ ë°›ì•˜ì–´ìš”.',\n",
              " 'ê·¸ëŸ¼ ê³ ì–‘ì´ë“¤ë¼ë¦¬ ì•ˆ ì‹¸ìš°ë‚˜ìš”?',\n",
              " 'ì €í¬ ì¼ê³±ì€ ë‹¤ê°™ì´ í•œ ê°€ì¡±ì…ë‹ˆë‹¤. ì‹¸ìš°ëŠ” ì¼ì€ ì—†ì–´ìš”.']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(session_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSKUlUoZmUAC",
        "outputId": "9d15ca77-532f-46e2-e93e-746bbccbe933"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "236"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ëª¨ë¸ì„ í•™ìŠµí•˜ê¸° ìœ„í•œ loss "
      ],
      "metadata": {
        "id": "hHYDlQ8Omx7W"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UPrhA7rZmiRR"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLMì„ ìœ„í•œ ì…ë ¥"
      ],
      "metadata": {
        "id": "c-7PP6JUnrXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " from transformers import AutoTokenizer\n",
        " tokenizer = AutoTokenizer.from_pretrained('klue/roberta-base')"
      ],
      "metadata": {
        "id": "X8BL0KIAnstl"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.special_tokens_map_extended"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXlRX0T-sZX6",
        "outputId": "7e54000a-f950-417c-8fff-37be8433db1d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bos_token': '[CLS]',\n",
              " 'eos_token': '[SEP]',\n",
              " 'unk_token': '[UNK]',\n",
              " 'sep_token': '[SEP]',\n",
              " 'pad_token': '[PAD]',\n",
              " 'cls_token': '[CLS]',\n",
              " 'mask_token': '[MASK]'}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.eos_token, tokenizer.sep_token, tokenizer.eos_token_id, tokenizer.sep_token_id"
      ],
      "metadata": {
        "id": "A2l8WIxwsgBt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d2c1d84-a02d-489c-b72e-e29f572bdbe9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('[SEP]', '[SEP]', 2, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "ë…¼ë¬¸ ì…ë ¥ í˜•íƒœ [CLS] u1 [EOU] ... ut-1 [EOU] [SEP] ut [SEP]\n",
        "sep_tokenì„ ë‹¤ë¥¸ í† í°ìœ¼ë¡œ ë³€ê²½í•˜ì\n",
        "-> ì…ë ¥í˜•íƒœ : [CLS] u1 <SEP> ... ut-1 <SEP> [SEP] ut [SEP]\n",
        "\"\"\"\n",
        "\n",
        "special_tokens = {'sep_token' : '<SEP>'}\n",
        "tokenizer.add_special_tokens(special_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjHrpo9C-7MA",
        "outputId": "5c05a1ac-b5ce-4655-c361-e13a17de78f5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.eos_token, tokenizer.sep_token, tokenizer.eos_token_id, tokenizer.sep_token_id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-9fDfasE6SD",
        "outputId": "bc959ade-e22f-4b15-fef3-6a4b8df06362"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('[SEP]', '<SEP>', 2, 32000)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.mask_token, tokenizer.mask_token_id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTheubcnE8_z",
        "outputId": "7ccea7ec-9b0f-4230-a21d-7d83cc36c36c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('[MASK]', 4)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "session = session_dataset[0]\n",
        "mask_ratio = 0.15\n",
        "corrupt_tokens = []\n",
        "output_tokens = []\n",
        "\n",
        "for i, utt in enumerate(session):\n",
        "    original_token = tokenizer.encode(utt, add_special_tokens=False)\n",
        "\n",
        "    mask_num = int(len(original_token)*mask_ratio)\n",
        "    mask_positions = random.sample([x for x in range(len(original_token))], mask_num)\n",
        "    corrupt_token = []\n",
        "    for pos in range(len(original_token)):\n",
        "        if pos in mask_positions:\n",
        "            corrupt_token.append(tokenizer.mask_token_id)\n",
        "        else:\n",
        "            corrupt_token.append(original_token[pos])\n",
        "            \n",
        "    if i == len(session)-1:\n",
        "        output_tokens += original_token\n",
        "        corrupt_tokens += corrupt_token\n",
        "    else:\n",
        "        output_tokens += original_token + [tokenizer.sep_token_id]\n",
        "        corrupt_tokens += corrupt_token + [tokenizer.sep_token_id]"
      ],
      "metadata": {
        "id": "fMa1ms4LFYbC"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(output_tokens))\n",
        "print('###')\n",
        "print(tokenizer.decode(corrupt_tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWf2e4hpHBPF",
        "outputId": "92b530e0-26d3-4cb7-db11-2165837269b0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì•ˆë…•í•˜ì„¸ìš”. ì €ëŠ” ê³ ì–‘ì´ 6ë§ˆë¦¬ í‚¤ì›Œìš”. <SEP> ê³ ì–‘ì´ë¥¼ 6ë§ˆë¦¬ë‚˜ìš”? í‚¤ìš°ëŠ”ê±° ì•ˆ í˜ë“œì„¸ìš”? <SEP> ì œê°€ ì›Œë‚™ ê³ ì–‘ì´ë¥¼ ì¢‹ì•„í•´ì„œ í¬ê²Œ í˜ë“¤ì§„ ì•Šì•„ìš”. <SEP> ê°€ì¥ ë‚˜ì´ê°€ ë§ì€ ê³ ì–‘ì´ê°€ ì–´ë–»ê²Œ ë¼ìš”? <SEP> ì—¬ì„¯ ì‚´ì…ë‹ˆë‹¤. ê°ˆìƒ‰ ê³ ì–‘ì´ì—ìš”. <SEP> ê·¸ëŸ¼ ê°€ì¥ ì–´ë¦° ê³ ì–‘ì´ê°€ ì–´ë–»ê²Œ ë¼ìš”? <SEP> í•œ ì‚´ì…ë‹ˆë‹¤. ì‘ë…„ì— ë¶„ì–‘ ë°›ì•˜ì–´ìš”. <SEP> ê·¸ëŸ¼ ê³ ì–‘ì´ë“¤ë¼ë¦¬ ì•ˆ ì‹¸ìš°ë‚˜ìš”? <SEP> ì €í¬ ì¼ê³±ì€ ë‹¤ê°™ì´ í•œ ê°€ì¡±ì…ë‹ˆë‹¤. ì‹¸ìš°ëŠ” ì¼ì€ ì—†ì–´ìš”.\n",
            "###\n",
            "ì•ˆë…•í•˜ì„¸ìš”. ì €ëŠ” ê³ ì–‘ì´ 6 [MASK] í‚¤ì›Œìš”. <SEP> ê³ ì–‘ì´ë¥¼ 6ë§ˆë¦¬ë‚˜ìš”? í‚¤ìš°ëŠ”ê±° ì•ˆ [MASK]ì„¸ìš” [MASK] <SEP> ì œê°€ ì›Œë‚™ ê³ ì–‘ì´ë¥¼ ì¢‹ì•„í•´ì„œ [MASK]ê²Œ í˜ë“¤ì§„ ì•Š [MASK]ìš”. <SEP> ê°€ì¥ ë‚˜ì´ê°€ ë§ì€ ê³ ì–‘ì´ [MASK] ì–´ë–»ê²Œ ë¼ìš”? <SEP> ì—¬ì„¯ ì‚´ì…ë‹ˆë‹¤. [MASK] ê³ ì–‘ì´ì—ìš”. <SEP> ê·¸ëŸ¼ ê°€ì¥ [MASK] ê³ ì–‘ì´ê°€ ì–´ë–»ê²Œ ë¼ìš”? <SEP> [MASK] ì‚´ì…ë‹ˆë‹¤. ì‘ë…„ì— ë¶„ì–‘ ë°›ì•˜ì–´ìš”. <SEP> ê·¸ëŸ¼ [MASK]ë“¤ë¼ë¦¬ ì•ˆ ì‹¸ìš°ë‚˜ìš”? <SEP> ì €í¬ ì¼ê³±ì€ ë‹¤ê°™ì´ í•œ ê°€ì¡±ì…ë‹ˆë‹¤ [MASK] ì‹¸ìš°ëŠ” ì¼ì€ ì—†ì–´ìš” [MASK]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## URCë¥¼ ìœ„í•œ ì…ë ¥ í† í°"
      ],
      "metadata": {
        "id": "zJalfojJWb0f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### short session êµ¬ì„±"
      ],
      "metadata": {
        "id": "PoNbqAKyXbgG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k =4 # ë…¼ë¬¸ ì°¸ì¡° (3ê°œëŠ” context, 1ê°œëŠ” response)\n",
        "short_session_dataset = []\n",
        "for session in session_dataset:\n",
        "      for i in range(len(session)-k+1):\n",
        "          short_session_dataset.append(session[i:i+k])\n",
        "print(len(short_session_dataset))"
      ],
      "metadata": {
        "id": "XsGInLYjHMrL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86047bbd-24e2-4277-c84f-03086455803a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2762\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "short_session_dataset[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0Iy-L4MXVaU",
        "outputId": "063dc5a9-bc51-4d48-ec16-aed04136d0d4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ì•ˆë…•í•˜ì„¸ìš”. ì €ëŠ” ê³ ì–‘ì´ 6ë§ˆë¦¬ í‚¤ì›Œìš”.',\n",
              " 'ê³ ì–‘ì´ë¥¼ 6ë§ˆë¦¬ë‚˜ìš”? í‚¤ìš°ëŠ”ê±° ì•ˆ í˜ë“œì„¸ìš”?',\n",
              " 'ì œê°€ ì›Œë‚™ ê³ ì–‘ì´ë¥¼ ì¢‹ì•„í•´ì„œ í¬ê²Œ í˜ë“¤ì§„ ì•Šì•„ìš”.',\n",
              " 'ê°€ì¥ ë‚˜ì´ê°€ ë§ì€ ê³ ì–‘ì´ê°€ ì–´ë–»ê²Œ ë¼ìš”?']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### URC ì…ë ¥ ë§Œë“¤ê¸°"
      ],
      "metadata": {
        "id": "Xv4kUnBSXd7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ëª¨ë“  ë°œí™”ê°€ negativ response candidate\n",
        "import random\n",
        "\n",
        "all_utts = set()\n",
        "for session in session_dataset:\n",
        "    for utt in session:\n",
        "        all_utts.add(utt)\n",
        "all_utts = list(all_utts)\n",
        "print(len(all_utts), all_utts[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RuUwmvHKW-Lu",
        "outputId": "f004fc1e-f7b5-43de-a693-9ceead708593"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3430 ì ìˆ˜í•¨ì´ë‚˜ ìŠ¤ë…¸í´ë§ ë„êµ¬ ê°™ì´ ë¹„ì‹¼ ë„êµ¬ ì—†ì´ ì–´ë–»ê²Œ ìˆ˜ë©´ ì•„ë˜ë¥¼ ê°ìƒí•˜ë‚˜ìš”?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "session = short_session_dataset[0]\n",
        "urc_tokens = []\n",
        "context_utts = []\n",
        "for i in range(len(session)):\n",
        "    utt = session[i]    \n",
        "    original_token = tokenizer.encode(utt, add_special_tokens=False)\n",
        "    if i == len(session)-1:\n",
        "        # urc_tokens += [tokenizer.eos_token_id]\n",
        "        \"\"\" ê¸°ì¡´ response ì…ë ¥ \"\"\" # ë§ˆì§€ë§‰ ë°œí™”\n",
        "        positive_tokens = urc_tokens + original_token \n",
        "        \"\"\" random negative respons ì…ë ¥ \"\"\" # randomìœ¼ë¡œ ì„ íƒí•´ì„œ ë„£ì–´ì¤€ë‹¤.\n",
        "        while True:\n",
        "            random_neg_response = random.choice(all_utts)\n",
        "            if random_neg_response not in context_utts:\n",
        "                break\n",
        "        random_neg_response_token = tokenizer.encode(random_neg_response, add_special_tokens=False)\n",
        "        random_tokens = urc_tokens + random_neg_response_token\n",
        "        \"\"\" context negative response ì…ë ¥ \"\"\" #context_uttsì—ì„œ ëœë¤ìœ¼ë¡œ í•˜ë‚˜ ë½‘ì•„ì¤€ë‹¤.\n",
        "        context_neg_response = random.choice(context_utts)\n",
        "        context_neg_response_token = tokenizer.encode(context_neg_response, add_special_tokens=False)\n",
        "        context_neg_tokens = urc_tokens + context_neg_response_token\n",
        "    else:\n",
        "        urc_tokens += original_token + [tokenizer.sep_token_id]\n",
        "    context_utts.append(utt)"
      ],
      "metadata": {
        "id": "M3_E8aMeXt1C"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(positive_tokens)) # 0\n",
        "print('#####')\n",
        "print(tokenizer.decode(random_tokens)) # 1\n",
        "print('#####')\n",
        "print(tokenizer.decode(context_neg_tokens)) # 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dxbg58i0Y88O",
        "outputId": "67d2db6a-79db-4556-a66e-914047615a28"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì•ˆë…•í•˜ì„¸ìš”. ì €ëŠ” ê³ ì–‘ì´ 6ë§ˆë¦¬ í‚¤ì›Œìš”. <SEP> ê³ ì–‘ì´ë¥¼ 6ë§ˆë¦¬ë‚˜ìš”? í‚¤ìš°ëŠ”ê±° ì•ˆ í˜ë“œì„¸ìš”? <SEP> ì œê°€ ì›Œë‚™ ê³ ì–‘ì´ë¥¼ ì¢‹ì•„í•´ì„œ í¬ê²Œ í˜ë“¤ì§„ ì•Šì•„ìš”. <SEP> ê°€ì¥ ë‚˜ì´ê°€ ë§ì€ ê³ ì–‘ì´ê°€ ì–´ë–»ê²Œ ë¼ìš”?\n",
            "#####\n",
            "ì•ˆë…•í•˜ì„¸ìš”. ì €ëŠ” ê³ ì–‘ì´ 6ë§ˆë¦¬ í‚¤ì›Œìš”. <SEP> ê³ ì–‘ì´ë¥¼ 6ë§ˆë¦¬ë‚˜ìš”? í‚¤ìš°ëŠ”ê±° ì•ˆ í˜ë“œì„¸ìš”? <SEP> ì œê°€ ì›Œë‚™ ê³ ì–‘ì´ë¥¼ ì¢‹ì•„í•´ì„œ í¬ê²Œ í˜ë“¤ì§„ ì•Šì•„ìš”. <SEP> ì €ëŠ” ì¶œê·¼í•˜ëŠ”ê²Œ ë„ˆë¬´ ì¦ê±°ì›Œìš”.\n",
            "#####\n",
            "ì•ˆë…•í•˜ì„¸ìš”. ì €ëŠ” ê³ ì–‘ì´ 6ë§ˆë¦¬ í‚¤ì›Œìš”. <SEP> ê³ ì–‘ì´ë¥¼ 6ë§ˆë¦¬ë‚˜ìš”? í‚¤ìš°ëŠ”ê±° ì•ˆ í˜ë“œì„¸ìš”? <SEP> ì œê°€ ì›Œë‚™ ê³ ì–‘ì´ë¥¼ ì¢‹ì•„í•´ì„œ í¬ê²Œ í˜ë“¤ì§„ ì•Šì•„ìš”. <SEP> ì•ˆë…•í•˜ì„¸ìš”. ì €ëŠ” ê³ ì–‘ì´ 6ë§ˆë¦¬ í‚¤ì›Œìš”.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ë°ì´í„° ë¡œë“œ ë§Œë“¤ê¸°"
      ],
      "metadata": {
        "id": "rYQDZPx3bSCF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import random\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "class post_loader(Dataset):\n",
        "    def __init__(self, data_path):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained('klue/roberta-base')\n",
        "        special_tokens = {'sep_token': '<SEP>'}\n",
        "        self.tokenizer.add_special_tokens(special_tokens)\n",
        "\n",
        "        f = open(data_path, 'r')\n",
        "        rdr = csv.reader(f)\n",
        "\n",
        "        \"\"\" ì„¸ì…˜ ë°ì´í„° ì €ì¥í•  ê²ƒ\"\"\"\n",
        "        session_dataset = []\n",
        "        session = []\n",
        "\n",
        "        \"\"\" ì‹¤ì œ ë°ì´í„° ì €ì¥ ë°©ì‹ \"\"\"\n",
        "        for i, line in enumerate(rdr):\n",
        "            if i == 0:\n",
        "                header = line\n",
        "            else:\n",
        "                utt = line[0] #formal\n",
        "                if utt.strip() != '':\n",
        "                    session.append(utt)\n",
        "                else:\n",
        "                    \"\"\" ì„¸ì…˜ ë°ì´í„° ì €ì¥ \"\"\"\n",
        "                    session_dataset.append(session)\n",
        "                    session = []\n",
        "        \"\"\" ë§ˆì§€ë§‰ ì„¸ì…˜ ì €ì¥ \"\"\"\n",
        "        session_dataset.append(session)\n",
        "        f.close()\n",
        "\n",
        "        \"\"\" short session context \"\"\"\n",
        "        self.short_session_dataset = []\n",
        "        for session in session_dataset:\n",
        "              for i in range(len(session)-k+1):\n",
        "                  self.short_session_dataset.append(session[i:i+k])\n",
        "\n",
        "        \"\"\" ëª¨ë“  ë°œí™” ì €ì¥ \"\"\"\n",
        "        self.all_utts = set()\n",
        "        for session in session_dataset:\n",
        "            for utt in session:\n",
        "                self.all_utts.add(utt)\n",
        "        self.all_utts = list(self.all_utts)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.short_session_dataset)\n",
        "\n",
        "    def __getitem__(self, idx): \n",
        "        session = self.short_session_dataset[idx]\n",
        "        \"\"\"MLMì…ë ¥\"\"\"\n",
        "        mask_ratio = 0.15\n",
        "        self.corrupt_tokens = []\n",
        "        self.output_tokens = []\n",
        "\n",
        "        for i, utt in enumerate(session):\n",
        "            original_token = self.tokenizer.encode(utt, add_special_tokens=False)\n",
        "\n",
        "            mask_num = int(len(original_token)*mask_ratio)\n",
        "            mask_positions = random.sample([x for x in range(len(original_token))], mask_num)\n",
        "            corrupt_token = []\n",
        "            for pos in range(len(original_token)):\n",
        "                if pos in mask_positions:\n",
        "                    corrupt_token.append(self.tokenizer.mask_token_id)\n",
        "                else:\n",
        "                    corrupt_token.append(original_token[pos])\n",
        "                    \n",
        "            if i == len(session)-1:\n",
        "                self.output_tokens += original_token\n",
        "                self.corrupt_tokens += corrupt_token\n",
        "            else:\n",
        "                self.output_tokens += original_token + [self.tokenizer.sep_token_id]\n",
        "                self.corrupt_tokens += corrupt_token + [self.tokenizer.sep_token_id]\n",
        "\n",
        "\n",
        "        \"\"\" label for loss \"\"\"\n",
        "        self.corrupt_mask_positions = []\n",
        "        for pos in range(len(self.corrupt_tokens)):\n",
        "            if self.corrupt_tokens[pos] == self.tokenizer.mask_token_id:\n",
        "                self.corrupt_mask_positions.append(pos)\n",
        "\n",
        "        \"\"\" URC ì…ë ¥ \"\"\"\n",
        "        urc_tokens = []\n",
        "        context_utts = []\n",
        "        for i in range(len(session)):\n",
        "            utt = session[i]    \n",
        "            original_token = self.tokenizer.encode(utt, add_special_tokens=False)\n",
        "            if i == len(session)-1:\n",
        "                urc_tokens += [self.tokenizer.eos_token_id]\n",
        "                \"\"\" ê¸°ì¡´ response ì…ë ¥ \"\"\" # ë§ˆì§€ë§‰ ë°œí™”\n",
        "                self.positive_tokens = [self.tokenizer.cls_token_id] + urc_tokens + original_token # cls token ì¶”ê°€ \n",
        "                \"\"\" random negative respons ì…ë ¥ \"\"\" # randomìœ¼ë¡œ ì„ íƒí•´ì„œ ë„£ì–´ì¤€ë‹¤.\n",
        "                while True:\n",
        "                    random_neg_response = random.choice(self.all_utts)\n",
        "                    if random_neg_response not in context_utts:\n",
        "                        break\n",
        "                random_neg_response_token = self.tokenizer.encode(random_neg_response, add_special_tokens=False)\n",
        "                self.random_tokens = [self.tokenizer.cls_token_id] + urc_tokens + random_neg_response_token\n",
        "                \"\"\" context negative response ì…ë ¥ \"\"\" #context_uttsì—ì„œ ëœë¤ìœ¼ë¡œ í•˜ë‚˜ ë½‘ì•„ì¤€ë‹¤.\n",
        "                context_neg_response = random.choice(context_utts)\n",
        "                context_neg_response_token = self.tokenizer.encode(context_neg_response, add_special_tokens=False)\n",
        "                self.context_neg_tokens = [self.tokenizer.cls_token_id] + urc_tokens + context_neg_response_token\n",
        "            else:\n",
        "                urc_tokens += original_token + [self.tokenizer.sep_token_id]\n",
        "            context_utts.append(utt)\n",
        "\n",
        "        return self.corrupt_tokens, self.output_tokens, self.corrupt_mask_positions, [self.positive_tokens, self.random_tokens, self.context_neg_tokens], [0,1,2]"
      ],
      "metadata": {
        "id": "ZdaOm5LXa9Ow"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = './korean_smile_style_dataset/smile.csv'\n",
        "post_dataset = post_loader(data_path)"
      ],
      "metadata": {
        "id": "6ndJicmMFktM"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corrupt_tokens, output_tokens, corrupt_mask_positions, urc_inputs, urc_labels = post_dataset[0]"
      ],
      "metadata": {
        "id": "KwFZTJFEG5cM"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(post_dataset.tokenizer.decode(corrupt_tokens))\n",
        "print(post_dataset.tokenizer.decode(output_tokens))\n",
        "print(corrupt_mask_positions) # ì´ ë¶€ë¶„ì— í•´ë‹¹í•˜ëŠ” lossë¥¼ ê³„ì‚°í•´ì•¼í•¨í•¨\n",
        "print('#####')\n",
        "print(post_dataset.tokenizer.decode(urc_inputs[0]))\n",
        "print(post_dataset.tokenizer.decode(urc_inputs[1]))\n",
        "print(post_dataset.tokenizer.decode(urc_inputs[2]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OK-oeJXkIUJx",
        "outputId": "360f2c81-76a0-4c1c-c8c3-a23010841295"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì•ˆë…•í•˜ [MASK]. ì €ëŠ” ê³ ì–‘ì´ 6ë§ˆë¦¬ í‚¤ì›Œìš”. <SEP> ê³ ì–‘ì´ë¥¼ 6ë§ˆë¦¬ë‚˜ [MASK]? í‚¤ìš°ëŠ”ê±° ì•ˆ [MASK]ì„¸ìš”? <SEP> ì œ [MASK] ì›Œë‚™ ê³ ì–‘ì´ë¥¼ ì¢‹ì•„í•´ì„œ í¬ê²Œ í˜ë“¤ì§„ ì•Šì•„ [MASK]. <SEP> ê°€ì¥ ë‚˜ì´ê°€ [MASK]ì€ ê³ ì–‘ì´ê°€ ì–´ë–»ê²Œ ë¼ìš”?\n",
            "ì•ˆë…•í•˜ì„¸ìš”. ì €ëŠ” ê³ ì–‘ì´ 6ë§ˆë¦¬ í‚¤ì›Œìš”. <SEP> ê³ ì–‘ì´ë¥¼ 6ë§ˆë¦¬ë‚˜ìš”? í‚¤ìš°ëŠ”ê±° ì•ˆ í˜ë“œì„¸ìš”? <SEP> ì œê°€ ì›Œë‚™ ê³ ì–‘ì´ë¥¼ ì¢‹ì•„í•´ì„œ í¬ê²Œ í˜ë“¤ì§„ ì•Šì•„ìš”. <SEP> ê°€ì¥ ë‚˜ì´ê°€ ë§ì€ ê³ ì–‘ì´ê°€ ì–´ë–»ê²Œ ë¼ìš”?\n",
            "[2, 18, 24, 29, 40, 46]\n",
            "#####\n",
            "[CLS] ì•ˆë…•í•˜ì„¸ìš”. ì €ëŠ” ê³ ì–‘ì´ 6ë§ˆë¦¬ í‚¤ì›Œìš”. <SEP> ê³ ì–‘ì´ë¥¼ 6ë§ˆë¦¬ë‚˜ìš”? í‚¤ìš°ëŠ”ê±° ì•ˆ í˜ë“œì„¸ìš”? <SEP> ì œê°€ ì›Œë‚™ ê³ ì–‘ì´ë¥¼ ì¢‹ì•„í•´ì„œ í¬ê²Œ í˜ë“¤ì§„ ì•Šì•„ìš”. <SEP> [SEP] ê°€ì¥ ë‚˜ì´ê°€ ë§ì€ ê³ ì–‘ì´ê°€ ì–´ë–»ê²Œ ë¼ìš”?\n",
            "[CLS] ì•ˆë…•í•˜ì„¸ìš”. ì €ëŠ” ê³ ì–‘ì´ 6ë§ˆë¦¬ í‚¤ì›Œìš”. <SEP> ê³ ì–‘ì´ë¥¼ 6ë§ˆë¦¬ë‚˜ìš”? í‚¤ìš°ëŠ”ê±° ì•ˆ í˜ë“œì„¸ìš”? <SEP> ì œê°€ ì›Œë‚™ ê³ ì–‘ì´ë¥¼ ì¢‹ì•„í•´ì„œ í¬ê²Œ í˜ë“¤ì§„ ì•Šì•„ìš”. <SEP> [SEP] ë„¤, ì´ì œ ìœ ì „ì˜ ê¸°ë³¸ ì›ë¦¬ì— ëŒ€í•´ ì¡°ê¸ˆ ì•Œ ê²ƒ ê°™ì•„ìš”.\n",
            "[CLS] ì•ˆë…•í•˜ì„¸ìš”. ì €ëŠ” ê³ ì–‘ì´ 6ë§ˆë¦¬ í‚¤ì›Œìš”. <SEP> ê³ ì–‘ì´ë¥¼ 6ë§ˆë¦¬ë‚˜ìš”? í‚¤ìš°ëŠ”ê±° ì•ˆ í˜ë“œì„¸ìš”? <SEP> ì œê°€ ì›Œë‚™ ê³ ì–‘ì´ë¥¼ ì¢‹ì•„í•´ì„œ í¬ê²Œ í˜ë“¤ì§„ ì•Šì•„ìš”. <SEP> [SEP] ê³ ì–‘ì´ë¥¼ 6ë§ˆë¦¬ë‚˜ìš”? í‚¤ìš°ëŠ”ê±° ì•ˆ í˜ë“œì„¸ìš”?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ë°°ì¹˜ ì²˜ë¦¬"
      ],
      "metadata": {
        "id": "Y0lMIsNIJGf4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import random\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "\n",
        "class post_loader(Dataset):\n",
        "    def __init__(self, data_path):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained('klue/roberta-base')\n",
        "        special_tokens = {'sep_token': '<SEP>'}\n",
        "        self.tokenizer.add_special_tokens(special_tokens)\n",
        "        \n",
        "        f = open(data_path, 'r')\n",
        "        rdr = csv.reader(f)        \n",
        "        \n",
        "        \"\"\" ì„¸ì…˜ ë°ì´í„° ì €ì¥í•  ê²ƒ\"\"\"\n",
        "        session_dataset = []\n",
        "        session = []\n",
        "\n",
        "        \"\"\" ì‹¤ì œ ë°ì´í„° ì €ì¥ ë°©ì‹ \"\"\"\n",
        "        for i, line in enumerate(rdr):\n",
        "            if i == 0:\n",
        "                header  = line\n",
        "            else:\n",
        "                utt = line[0]\n",
        "                if utt.strip() != '':\n",
        "                    session.append(utt)\n",
        "                else:\n",
        "                    \"\"\" ì„¸ì…˜ ë°ì´í„° ì €ì¥ \"\"\"\n",
        "                    session_dataset.append(session)\n",
        "                    session = []\n",
        "        \"\"\" ë§ˆì§€ë§‰ ì„¸ì…˜ ì €ì¥ \"\"\"\n",
        "        session_dataset.append(session)\n",
        "        f.close()\n",
        "        \n",
        "        \"\"\" short session context \"\"\"\n",
        "        k = 4 # ë…¼ë¬¸ì—ì„œ ê°€ì¥ ì¢‹ì•˜ë˜ ìˆ«ì\n",
        "        self.short_session_dataset = []\n",
        "        for session in session_dataset:    \n",
        "            for i in range(len(session)-k+1):\n",
        "                self.short_session_dataset.append(session[i:i+k])\n",
        "                \n",
        "        \"\"\" ëª¨ë“  ë°œí™” ì €ì¥ \"\"\"\n",
        "        self.all_utts = set()\n",
        "        for session in session_dataset:\n",
        "            for utt in session:\n",
        "                self.all_utts.add(utt)\n",
        "        self.all_utts = list(self.all_utts)\n",
        "        \n",
        "    def __len__(self): # ê¸°ë³¸ì ì¸ êµ¬ì„±\n",
        "        return len(self.short_session_dataset)\n",
        "    \n",
        "    def __getitem__(self, idx): # ê¸°ë³¸ì ì¸ êµ¬ì„±\n",
        "        session = self.short_session_dataset[idx]\n",
        "        \"\"\" MLM ì…ë ¥ \"\"\"\n",
        "        mask_ratio = 0.15\n",
        "        self.corrupt_tokens = []\n",
        "        self.output_tokens = []\n",
        "        for i, utt in enumerate(session):\n",
        "            original_token = self.tokenizer.encode(utt, add_special_tokens=False)\n",
        "\n",
        "            mask_num = int(len(original_token)*mask_ratio)\n",
        "            mask_positions = random.sample([x for x in range(len(original_token))], mask_num)\n",
        "            corrupt_token = []\n",
        "            for pos in range(len(original_token)):\n",
        "                if pos in mask_positions:\n",
        "                    corrupt_token.append(self.tokenizer.mask_token_id)\n",
        "                else:\n",
        "                    corrupt_token.append(original_token[pos])\n",
        "\n",
        "            if i == len(session)-1:\n",
        "                self.output_tokens += original_token\n",
        "                self.corrupt_tokens += corrupt_token\n",
        "            else:\n",
        "                self.output_tokens += original_token + [self.tokenizer.sep_token_id]\n",
        "                self.corrupt_tokens += corrupt_token + [self.tokenizer.sep_token_id]    \n",
        "        \n",
        "        \"\"\" label for loss \"\"\"\n",
        "        self.corrupt_mask_positions = []\n",
        "        for pos in range(len(self.corrupt_tokens)):\n",
        "            if self.corrupt_tokens[pos] == self.tokenizer.mask_token_id:\n",
        "                self.corrupt_mask_positions.append(pos)                \n",
        "                \n",
        "        \"\"\" URC ì…ë ¥ \"\"\"\n",
        "        urc_tokens = []\n",
        "        context_utts = []\n",
        "        for i in range(len(session)):\n",
        "            utt = session[i]    \n",
        "            original_token = self.tokenizer.encode(utt, add_special_tokens=False)\n",
        "            if i == len(session)-1:\n",
        "                urc_tokens += [self.tokenizer.eos_token_id]\n",
        "                \"\"\" ê¸°ì¡´ response ì…ë ¥ \"\"\"\n",
        "                self.positive_tokens = [self.tokenizer.cls_token_id] + urc_tokens + original_token\n",
        "                \"\"\" random negative respons ì…ë ¥ \"\"\"\n",
        "                while True:\n",
        "                    random_neg_response = random.choice(self.all_utts)\n",
        "                    if random_neg_response not in context_utts:\n",
        "                        break\n",
        "                random_neg_response_token = self.tokenizer.encode(random_neg_response, add_special_tokens=False)\n",
        "                self.random_tokens = [self.tokenizer.cls_token_id] + urc_tokens + random_neg_response_token\n",
        "                \"\"\" context negative response ì…ë ¥ \"\"\"\n",
        "                context_neg_response = random.choice(context_utts)\n",
        "                context_neg_response_token = self.tokenizer.encode(context_neg_response, add_special_tokens=False)\n",
        "                self.context_neg_tokens = [self.tokenizer.cls_token_id] + urc_tokens + context_neg_response_token\n",
        "            else:\n",
        "                urc_tokens += original_token + [self.tokenizer.sep_token_id]\n",
        "            context_utts.append(utt)\n",
        "        \n",
        "        return self.corrupt_tokens, self.output_tokens, self.corrupt_mask_positions, [self.positive_tokens, self.random_tokens, self.context_neg_tokens], [0, 1, 2]\n",
        "    \n",
        "    def collate_fn(self, sessions): # ë°°ì¹˜ë¥¼ ìœ„í•œ êµ¬ì„±\n",
        "        '''\n",
        "            input:\n",
        "                data: [(session1), (session2), ... ]\n",
        "            return:\n",
        "                batch_corrupt_tokens: (B, L) padded\n",
        "                batch_output_tokens: (B, L) padded\n",
        "                batch_corrupt_mask_positions: list\n",
        "                batch_urc_inputs: (B, L) padded\n",
        "                batch_urc_labels: (B)\n",
        "                batch_mlm_attentions\n",
        "                batch_urc_attentions\n",
        "\n",
        "            batchê°€ 3\n",
        "            MLM = 3ê°œì˜ ì…ë ¥ë°ì´í„° (ì…ë ¥ë°ì´í„°ë³„ë¡œ ê¸¸ì´ê°€ ë‹¤ë¦„)\n",
        "            URC = 9ê°œì˜ ì…ë ¥ë°ì´í„° (contextëŠ” ê¸¸ì´ê°€ ë‹¤ë¦„, response candidateë„ ê¸¸ì´ê°€ ë‹¤ë¦„)\n",
        "        '''\n",
        "        batch_corrupt_tokens, batch_output_tokens, batch_corrupt_mask_positions, batch_urc_inputs, batch_urc_labels = [], [], [], [], []\n",
        "        batch_mlm_attentions, batch_urc_attentions = [], []\n",
        "        # MLM, URC ì…ë ¥ì— ëŒ€í•´ì„œ ê°€ì¥ ê¸´ ì…ë ¥ ê¸¸ì´ë¥¼ ì°¾ê¸°\n",
        "        corrupt_max_len, urc_max_len = 0, 0\n",
        "        for session in sessions:\n",
        "            corrupt_tokens, output_tokens, corrupt_mask_positions, urc_inputs, urc_labels = session\n",
        "            if len(corrupt_tokens) > corrupt_max_len:\n",
        "                corrupt_max_len = len(corrupt_tokens)\n",
        "            positive_tokens, random_tokens, context_neg_tokens = urc_inputs\n",
        "            if max([len(positive_tokens), len(random_tokens), len(context_neg_tokens)]) > urc_max_len:\n",
        "                urc_max_len = max([len(positive_tokens), len(random_tokens), len(context_neg_tokens)])\n",
        "                \n",
        "        ## padding í† í°ì„ ì¶”ê°€í•˜ëŠ” ë¶€ë¶„\n",
        "        for session in sessions:\n",
        "            corrupt_tokens, output_tokens, corrupt_mask_positions, urc_inputs, urc_labels = session\n",
        "            \"\"\" mlm ì…ë ¥ \"\"\"\n",
        "            batch_corrupt_tokens.append(corrupt_tokens + [self.tokenizer.pad_token_id for _ in range(corrupt_max_len-len(corrupt_tokens))])\n",
        "            batch_mlm_attentions.append([1 for _ in range(len(corrupt_tokens))] + [0 for _ in range(corrupt_max_len-len(corrupt_tokens))])\n",
        "            \n",
        "            \"\"\" mlm ì¶œë ¥ \"\"\"\n",
        "            batch_output_tokens.append(output_tokens + [self.tokenizer.pad_token_id for _ in range(corrupt_max_len-len(corrupt_tokens))])\n",
        "            \n",
        "            \"\"\" mlm ë ˆì´ë¸” \"\"\"\n",
        "            batch_corrupt_mask_positions.append(corrupt_mask_positions)\n",
        "            \n",
        "            \"\"\" urc ì…ë ¥ \"\"\"\n",
        "            # positive_tokens, random_tokens, context_neg_tokens = urc_inputs\n",
        "            for urc_input in urc_inputs:                            \n",
        "                batch_urc_inputs.append(urc_input + [self.tokenizer.pad_token_id for _ in range(urc_max_len-len(urc_input))])\n",
        "                batch_urc_attentions.append([1 for _ in range(len(urc_input))] + [0 for _ in range(urc_max_len-len(urc_input))])\n",
        "            \n",
        "            \"\"\" urc ë ˆì´ë¸” \"\"\"\n",
        "            batch_urc_labels += urc_labels\n",
        "        return torch.tensor(batch_corrupt_tokens), torch.tensor(batch_output_tokens), batch_corrupt_mask_positions, torch.tensor(batch_urc_inputs), \\\n",
        "        torch.tensor(batch_urc_labels), torch.tensor(batch_mlm_attentions), torch.tensor(batch_urc_attentions)"
      ],
      "metadata": {
        "id": "g_PmGszrIpa4"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "data_path = './korean_smile_style_dataset/smile.csv'\n",
        "post_dataset = post_loader(data_path)\n",
        "post_dataloader = DataLoader(post_dataset, batch_size=2, shuffle=True, num_workers=4, collate_fn=post_dataset.collate_fn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWa2cJE-NBzu",
        "outputId": "565d42d0-a1a2-4641-eb40-10128a3913f7"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for data in post_dataloader:\n",
        "    batch_corrupt_tokens, batch_output_tokens, batch_corrupt_mask_positions, batch_urc_inputs, batch_urc_labels, batch_mlm_attentions, batch_urc_attentions = data\n",
        "    break"
      ],
      "metadata": {
        "id": "giqYxEUHPaKy"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_corrupt_tokens.shape, batch_output_tokens.shape, batch_corrupt_mask_positions, batch_urc_inputs.shape, batch_urc_labels.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8a2s9uSHQ2n4",
        "outputId": "e370f265-843f-45c1-ffdb-eda438db80cf"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([2, 50]),\n",
              " torch.Size([2, 50]),\n",
              " [[1, 14, 28, 33, 46], [4, 16, 28, 35]],\n",
              " torch.Size([6, 62]),\n",
              " torch.Size([6]))"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "post_dataset.tokenizer.decode(batch_corrupt_tokens[0,:]), post_dataset.tokenizer.decode(batch_output_tokens[0,:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1k8G9Q4RCBH",
        "outputId": "55a6f848-d4aa-4a55-871e-2804c7a997cb"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('ì˜êµ­ [MASK] ë­˜ í• ì§€ ë²Œì¨ë¶€í„° ê¸°ëŒ€ë˜ë„¤ìš”. <SEP> ì˜êµ­ ìŒì‹ [MASK] ê·¸ë ‡ê²Œ ë§›ìˆë°ìš”. <SEP> ê·¸ëŸ°ê°€ìš”? ì¼ë‹¨ ë¨¹ê³  ì‹¶ì€ [MASK]ì„ ëª‡ ê°œ ì •ë¦¬ [MASK]ë’€ìŠµë‹ˆë‹¤. <SEP> ê·¸ë¦¬ê³  ì˜êµ­ì˜ ê´€ê´‘ì§€ëŠ” ë‹¤ ë³´ [MASK]ì•¼ì£ .',\n",
              " 'ì˜êµ­ì—ì„œ ë­˜ í• ì§€ ë²Œì¨ë¶€í„° ê¸°ëŒ€ë˜ë„¤ìš”. <SEP> ì˜êµ­ ìŒì‹ì´ ê·¸ë ‡ê²Œ ë§›ìˆë°ìš”. <SEP> ê·¸ëŸ°ê°€ìš”? ì¼ë‹¨ ë¨¹ê³  ì‹¶ì€ ìŒì‹ì„ ëª‡ ê°œ ì •ë¦¬í•´ë’€ìŠµë‹ˆë‹¤. <SEP> ê·¸ë¦¬ê³  ì˜êµ­ì˜ ê´€ê´‘ì§€ëŠ” ë‹¤ ë³´ì…”ì•¼ì£ .')"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "post_dataset.tokenizer.decode(batch_corrupt_tokens[0,:]), post_dataset.tokenizer.decode(batch_output_tokens[1,:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5uyqYHvRDPn",
        "outputId": "de3d26ef-23ef-4daa-a3c4-040090403fef"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('ì˜êµ­ [MASK] ë­˜ í• ì§€ ë²Œì¨ë¶€í„° ê¸°ëŒ€ë˜ë„¤ìš”. <SEP> ì˜êµ­ ìŒì‹ [MASK] ê·¸ë ‡ê²Œ ë§›ìˆë°ìš”. <SEP> ê·¸ëŸ°ê°€ìš”? ì¼ë‹¨ ë¨¹ê³  ì‹¶ì€ [MASK]ì„ ëª‡ ê°œ ì •ë¦¬ [MASK]ë’€ìŠµë‹ˆë‹¤. <SEP> ê·¸ë¦¬ê³  ì˜êµ­ì˜ ê´€ê´‘ì§€ëŠ” ë‹¤ ë³´ [MASK]ì•¼ì£ .',\n",
              " 'ì•„ì¹¨ì„ ë“œì‹œë‚˜ìš”? <SEP> ì•„ë‹ˆìš”, ì €ëŠ” ì•„ì¹¨ì„ ì•ˆ ë¨¹ìŠµë‹ˆë‹¤. <SEP> ì €ëŠ” ë§¤ì¼ ì•„ì¹¨ ì‹œë¦¬ì–¼ í•œ ê·¸ë¦‡ì„ ë¨¹ê³  ë‚˜ì™€ìš”. <SEP> ì‹œë¦¬ì–¼ í•œ ê·¸ë¦‡ì„ ë¨¹ìœ¼ë©´ ë°°ê°€ ì°¨ë‚˜ìš”? [PAD] [PAD] [PAD] [PAD] [PAD]')"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_mlm_attentions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crS2bUy6REH-",
        "outputId": "1bdc4a44-c725-43c3-82ff-6e2c04b8b7ec"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
              "         0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "post_dataset.tokenizer.decode(batch_urc_inputs[0]), batch_urc_labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2mSKRVvRFDH",
        "outputId": "6d7a91e7-d5bf-4c3c-a446-bed3814f15a3"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('[CLS] ì˜êµ­ì—ì„œ ë­˜ í• ì§€ ë²Œì¨ë¶€í„° ê¸°ëŒ€ë˜ë„¤ìš”. <SEP> ì˜êµ­ ìŒì‹ì´ ê·¸ë ‡ê²Œ ë§›ìˆë°ìš”. <SEP> ê·¸ëŸ°ê°€ìš”? ì¼ë‹¨ ë¨¹ê³  ì‹¶ì€ ìŒì‹ì„ ëª‡ ê°œ ì •ë¦¬í•´ë’€ìŠµë‹ˆë‹¤. <SEP> [SEP] ê·¸ë¦¬ê³  ì˜êµ­ì˜ ê´€ê´‘ì§€ëŠ” ë‹¤ ë³´ì…”ì•¼ì£ . [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " tensor([0, 1, 2, 0, 1, 2]))"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## íŒŒì¼ë¡œ ì €ì¥í•˜ê¸°"
      ],
      "metadata": {
        "id": "92oQJuunRyhm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!touch dataset.py"
      ],
      "metadata": {
        "id": "ImHwhVFwRJHQ"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dataset import post_loader\n",
        "from torch.utils.data import DataLoader\n",
        "data_path = './korean_smile_style_dataset/smile.csv'\n",
        "post_dataset = post_loader(data_path)\n",
        "post_dataloader = DataLoader(post_dataset, batch_size=2, shuffle=True, num_workers=4, collate_fn=post_dataset.collate_fn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USEbuHgXR4DP",
        "outputId": "a4589b17-1c9a-4d83-b2b9-4f23affdd382"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for data in post_dataloader:\n",
        "    batch_corrupt_tokens, batch_output_tokens, batch_corrupt_mask_positions, batch_urc_inputs, batch_urc_labels, batch_mlm_attentions, batch_urc_attentions = data\n",
        "    break"
      ],
      "metadata": {
        "id": "HiVwYiR5SGKP"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_corrupt_tokens.shape, batch_output_tokens.shape, batch_corrupt_mask_positions, batch_urc_inputs.shape, batch_urc_labels.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVlrRCEASIov",
        "outputId": "596be556-2553-4ae2-c345-e94a1ce3fc93"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([2, 61]),\n",
              " torch.Size([2, 61]),\n",
              " [[4, 22, 35, 39, 48], [7, 14, 28, 34, 41, 53]],\n",
              " torch.Size([6, 89]),\n",
              " torch.Size([6]))"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ëª¨ë¸ë§í•˜ê¸°"
      ],
      "metadata": {
        "id": "Eay00ZL0SVGG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5iPDiq4oSJ35"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}